---
title: "678 Project"
author: "Aabha Latkar"
format: html
editor: visual
---

## Starbucks! Introduction to the Dataset

This dataset was released by Starbucks and is a simulation of actual events recorded on the Starbucks app. It has 3 files- 1. profile: Has information about the customer profiles including demographic information like their age, income, gender and when they became a Starbucks rewards member. 2. portfolio: Has information about the offers that were sent to customers. There are 10 offers which can either be BOGO- buy one get one free, discount and informational. The offers also have different difficulty levels, rewards and channels through which they are distributed. 3. transcript: This is the data about the events recorded on the app. There is information about customers receiving, viewing and completing offers. It also has data about the transactions that took place.

Goal: The objective of this analysis is to build a logistic regression that predicts if a customer will complete an offer that was sent out.

First, we will clean the data and merge the 3 files together so that we have demographic information about a customer who accepted/ rejected an offer and the characteristics of the offer.

The offers differ in "lucrativity" and popularity; offers fon't have the same rate of completion. Some offers are completed more than the other. There is variation by offers on the rate of completion. That is why, we will build a multilevel model where we will vary the intercept by the offers.

We shall also see if the relationship between the predictors and outcome variable changes with the offers. If it does, we can do a varying slope model to capture that variation. we can see that only 26% of the offers in our random sample were completed. Sending the offers requires money and if we can predict beforehand if a customer will complete the offer, we can save money on advertising and take business decisions accordingly.

Starbucks <https://quarto.org>.

## Objectives and Methods

When you click the **Render**

#the chi-square/box plots for y and correlation #more samples #confusion matrix #check the code of misclassi #no pooling #literature review

## Data Wrangling

```{r, echo=FALSE, results='hide'}

#First we will load the data and the libraries
library(tidyr)
library(readr)
library(lubridate)
library(stringr)
library(reshape2)
library(ggplot2)
library(lme4)
library(dplyr)
library(caret)
library(ROSE)
library(tibble)

portfolio <- read_csv("portfolio.csv", show_col_types = FALSE)
profile <- read_csv("profile.csv", show_col_types = FALSE)
transcript <- read_csv("transcript2.csv", show_col_types = FALSE)
```

```{r, echo=FALSE}
#Get the data description and see if there are any missing values for each of the 3 dataset

#profile
#summary(profile)
#we can see that there are a lot of missing values in the income variable and there is 
#one incorrect observation- age 118. We can fix that. We will also have to seperate the "became member on" column into year, month and day to get the duration of the customer's membership. 
profile <- na.omit(profile)

profile <- subset(profile, age != 118)
profile <- subset(profile, age !=101)

profile$became_member_on <- ymd(profile$became_member_on)
profile$weeks_as_member <- as.numeric(difftime(Sys.Date(), profile$became_member_on, units = "weeks"))
profile$weeks_as_member <- round(profile$weeks_as_member)

colnames(profile)[colnames(profile) == "id"] <- "person"
profile <- profile[, -1]

#portfolio
#summary(portfolio)
portfolio <- portfolio[, -1]

#transcript
#summary(transcript)
transcript <- transcript[, -1]
transcript <- transcript %>% select(-reward)
#merge the datasets together
#we can merge transcript and portfolio by id and proile and transcript by person
data1 <- merge(profile, transcript, by = "person", all.x = FALSE)
data1$new <- 1
data1 <- data1 %>%
  pivot_wider(names_from = event, values_from = new, values_fn = list)
starbucks <- merge(data1, portfolio, by = "id", all.x = TRUE)
starbucks <- starbucks %>%
  select( -10, -11)
starbucks[is.na(starbucks)] <- 0
income_brackets <- cut(starbucks$income, breaks = c(30000, 50000, 70000, 90000, 110000, 120000),
                       labels = c("30k-50k", "50k-70k", "70k-90k", "90k-110k", "110k-120k"),
                       include.lowest = TRUE)
starbucks$income_bracket <- income_brackets
starbucks <- starbucks %>% filter(transaction != "1")
starbucks <- starbucks %>% select(-transaction, -`transaction amount`, -`offer viewed`, -`offer received`, -time)
starbucks$`offer completed`[is.null(starbucks$`offer completed`)] <- 0
starbucks$became_member_on <- as.Date(starbucks$became_member_on, format = "%Y%m%d")
starbucks$member_year <- year(starbucks$became_member_on)
starbucks <- starbucks %>% select(-became_member_on)
starbucks$`offer completed` <- gsub("NULL", "0", starbucks$`offer completed`)
starbucks<- starbucks[starbucks$`offer completed` %in% c(0, 1), ]
starbucks <- starbucks %>%
                    group_by(person) %>%
                    sample_n(size = 1) %>%
                    ungroup()
ids_without_offer_completed <- starbucks %>%
  group_by(id) %>%
  summarise(has_offer_completed = any(`offer completed` == 1)) %>%
  filter(!has_offer_completed) %>%
  pull(id)
starbucks <- starbucks %>%
  filter(!(id %in% ids_without_offer_completed))
starbucks$age <- scale(starbucks$age)
starbucks$income <- scale(starbucks$income)
starbucks$weeks_as_member <- scale(starbucks$weeks_as_member)
starbucks<- starbucks%>%
  rename(completed = `offer completed`)
```

##EDA

```{r, echo=FALSE}
#correlation analysis
numeric_columns <- sapply(starbucks, is.numeric)
numeric_starbucks <- starbucks[, numeric_columns]
cormat <- round(cor(numeric_starbucks), 2)
#head(cormat)
melted_cormat <- melt(cormat)

ggplot(data = melted_cormat, aes(Var1, Var2, fill = value)) +   geom_tile() +   geom_text(aes(label = round(value, 2)), color = "black") +     scale_fill_gradient(low = "blue", high = "red") +   labs(title = "College Correlation Heatmap", x = "Variables", y = "Variables") +   theme(axis.text.x = element_text(angle = 45, hjust = 1))  

```

#We can conclude some things from the correlation map: 1. Member year and weeks_as_member have a high negative correlation, so they cannot both be used. I am choosing to use weeks_as_member as one of my predictors as it is a continuous variable and



```{r, echo=FALSE}
# Group by ID and summarize completion counts
proportion_data <- starbucks %>%
  group_by(id) %>%
  summarise(count = sum(completed == 1))

# Calculate proportions
proportion_data1 <- proportion_data %>% 
  mutate(proportion = count / sum(count))
# Create bar plot for the factor variable `completed`
ggplot(proportion_data1, aes(x = as.factor(id), y = proportion,fill = proportion)) +
  geom_bar(stat = "identity") +
  labs(title = "Proportion of Offer Completion by ID",
       x = "ID",
       y = "Proportion of Offer Completion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r, echo=FALSE}
starbucks$completed <- factor(starbucks$completed, levels = c(0, 1))
ggplot(starbucks, aes(x = completed, y = income, fill = completed)) +
  geom_boxplot() +
  labs(title = "Side-by-Side Box Plot",
       x = "Offer completed",
       y = "Income") +
  scale_fill_manual(values = c("darkgreen", "lightgreen"))
```

```{r, echo=FALSE}
ggplot(starbucks, aes(x = completed, y = age, fill = completed)) +
  geom_boxplot() +
  labs(title = "Side-by-Side Box Plot",
       x = "Binary Variable",
       y = "Age") +
  scale_fill_manual(values = c("darkblue", "lightblue"))
```

```{r, echo=FALSE}
ggplot(starbucks, aes(x = completed, y = weeks_as_member, fill = completed)) +
  geom_boxplot() +
  labs(title = "Side-by-Side Box Plot",
       x = "offer completed",
       y = "Weeks as Member") +
  scale_fill_manual(values = c("violet", "pink"))
```

```{r, echo=FALSE}

percentage_data <- starbucks %>%
  group_by(gender, completed) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

ggplot(percentage_data, aes(x = factor(gender), y = percentage, fill = factor(completed))) +
  geom_bar(stat = "identity", position = "fill") +
  labs(x = "Gender", y = "Percentage") +
  scale_fill_manual(values = c("0" = "darkblue", "1" = "lightblue")) +
  theme_minimal()
```

```{r, echo=FALSE}
percentage_data2 <- starbucks %>%
  group_by(offer_type, completed) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

ggplot(percentage_data2, aes(x = factor(offer_type), y = percentage, fill = factor(completed))) +
  geom_bar(stat = "identity", position = "fill") +
  labs(x = "Offer_type", y = "Percentage") +
  scale_fill_manual(values = c("0" = "darkblue", "1" = "lightblue")) +
  theme_minimal()
```

```{r, echo=FALSE}
ggplot(starbucks, aes(x = factor(completed), y = weeks_as_member, fill = factor(completed))) +
  geom_violin(scale = "width", trim = FALSE) +
  facet_wrap(~id, scales = "free_y") +
  labs(title = "Weeks_as_member by 'offer completed'",
       x = "completed", y = "Income") +
  theme_minimal()
```


```{r, echo=FALSE}
set.seed(123)
# 80% of the data will be used for training
index <- createDataPartition(starbucks$completed, p = 0.8, list = FALSE)

# Create training and test sets
starbucks_train <- starbucks[index, ]
starbucks_test  <- starbucks[-index, ]
```

```{r, echo=FALSE}

# Perform oversampling using ovun.sample
starbucks <- ovun.sample(completed ~ age + gender + income + reward + id+ difficulty + weeks_as_member + offer_type + channels, data = starbucks_train, method = "both", N = 12000, seed =1)$data


# Check the class distribution after oversampling
table(starbucks$completed)

```
##Modelling!

```{r}
#The null model
null_model <- glm(completed~ 1, data= starbucks_train, family= "binomial")
summary(null_model)
```

```{r}
#The normal model
try1 <- glm(completed ~ income + age + gender + gender:income + gender:age + weeks_as_member+ difficulty+reward, data = starbucks_train, family = "binomial")
summary(try1)
```

```{r}
#partial_pooling1
partial_pool_model <- glmer(formula = completed~ age + income + gender + weeks_as_member + (1 | id),
                        family = binomial,
                        data = starbucks_train)
summary(partial_pool_model)

```

```{r}
try3 <- glm(completed ~ income, data = starbucks_train, family = "binomial")
summary(try3)
```

```{r}
#partial_pooling2
partial_pool_model2 <- glmer(formula = completed~ income + age + gender + gender:income + gender:age + weeks_as_member + offer_type + difficulty +(1 | id),
                        family = binomial,
                        data = starbucks_train)
summary(partial_pool_model)

```

```{r}
#varying slope
partial_pooling3<- glmer(formula = completed~ income + age + gender + gender:income + gender:age + weeks_as_member  +(1+weeks_as_member| id),family = binomial, data = starbucks_train)
partial_pooling3
```

```{r}
#complete pooling
cp <- glm(completed ~ income + age + gender + gender:income + gender:age + weeks_as_member+id, data = starbucks_train, family = "binomial")
summary(cp)
#ask if you add group level predictors to complete pooling
```

```{r}
model_list <- list(null_model, try1,partial_pool_model,partial_pool_model2, cp)
```

```{r, , echo=FALSE}

calculate_misclassification_errors <- function(model_list, starbucks_test) {
  # Initialize an empty list to store results
  results <- list()

  # Loop through each model
  for (i in seq_along(model_list)) {
    # Get the model
    model <- model_list[[i]]

    # Get the model name
    model_name <- names(model_list)[i]

    # Predict probabilities
    predicted_probabilities <- predict(model, newdata = starbucks_test, type = "response")

    # Convert probabilities to binary classes
    predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)

    # Get actual classes from test data
    actual_classes <- starbucks_test$completed

    # Calculate misclassification error
    misclassification_error <- mean(predicted_classes != actual_classes)

    # Store results in a list
    results[[i]] <- tibble(Model = model_name, Misclassification_Error = misclassification_error)
  }

  # Combine results into a single data frame
  result_table <- do.call(rbind, results)

  return(result_table)
}
calculate_misclassification_errors(model_list, starbucks_test)

```

```{r}
try1 <- glm(completed ~ income + age + gender + gender:income + gender:age + weeks_as_member+ difficulty+reward, data = starbucks_train, family = "binomial")
null_model <- glm(completed~ 1, data= starbucks_train, family= "binomial")

predicted_probabilities <- predict(try3, newdata = starbucks_test, type = "response")
predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)
 actual_classes <- starbucks_test$completed
conf_matrix <- confusionMatrix(factor(predicted_classes), factor(actual_classes))
conf_matrix
```

```{r}
# Assuming you have a list of models named model_list
model_list <- list(try1, null_model, partial_pool_model, partial_pool_model2, cp)

# Create an empty list to store confusion matrices
confusion_matrices <- list()

# Loop through each model in the list
for (model in model_list) {
  # Make predictions using the current model
  predicted_probabilities <- predict(model, newdata = starbucks_test, type = "response")
  predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)
  
  # Get actual classes from the test data
  actual_classes <- starbucks_test$completed
  
  # Create confusion matrix
  conf_matrix <- confusionMatrix(factor(predicted_classes), factor(actual_classes))
  
  # Append the confusion matrix to the list
  confusion_matrices <- c(confusion_matrices, list(conf_matrix))
}

# Print the confusion matrices
for (i in seq_along(model_list)) {
  cat("Confusion Matrix for Model", i, ":\n")
  print(confusion_matrices[[i]])
  cat("\n")
}

```

